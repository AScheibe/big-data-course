{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da650ca-0501-4f15-b888-d892e81c17ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.20.0.3  70.27 KiB   16      65.5%             0ab6961e-a9d3-42f4-91d1-7db3e83ef130  rack1\n",
      "UN  172.20.0.4  242.43 KiB  16      66.4%             d2f07131-7241-47da-ade9-6dd65ba26510  rack1\n",
      "UN  172.20.0.2  70.26 KiB   16      68.0%             01024492-5ed5-4823-848b-fb124b354be2  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60eef89-954a-46d3-8828-aeaa31a3bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b517b7c-2539-4f3b-a5d6-2ead6cba1988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f83e5b37430>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"drop keyspace if exists weather\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "create keyspace weather\n",
    "with replication = {'class': 'SimpleStrategy', 'replication_factor': 3}\n",
    "\"\"\")\n",
    "\n",
    "cass.set_keyspace('weather')\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "create type station_record (\n",
    "    tmin int,\n",
    "    tmax int\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "CREATE TABLE stations (\n",
    "    id text,\n",
    "    name text STATIC,\n",
    "    date date,\n",
    "    record station_record,\n",
    "    PRIMARY KEY ((id), date)\n",
    ") WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7fa076-9c74-4c6a-806c-e4f1559804ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "cass.execute(\"describe table weather.stations\").one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647d4244-f061-4f96-89b7-dce872dbc90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7f09870f-c640-40fa-8b87-b4e366c2ccfd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (147ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (91ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (285ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (33ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (52ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (28ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (139ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (37ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (116ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (37ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (34ms)\n",
      ":: resolution report :: resolve 4915ms :: artifacts dl 1253ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7f09870f-c640-40fa-8b87-b4e366c2ccfd\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/92ms)\n",
      "24/04/19 04:25:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf542c4-56f8-4f27-ad5b-3645cf425b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----+\n",
      "|         ID|               NAME|STATE|\n",
      "+-----------+-------------------+-----+\n",
      "|US1WIAD0002|        ADAMS 0.4 E|   WI|\n",
      "|US1WIAD0005|    NEKOOSA 8.0 SSE|   WI|\n",
      "|US1WIAD0006|  GRAND MARSH 1.0 W|   WI|\n",
      "|US1WIAD0008|GRAND MARSH 1.9 SSW|   WI|\n",
      "|US1WIAD0010|       OXFORD 4.0 W|   WI|\n",
      "|US1WIAD0015|       OXFORD 4.3 W|   WI|\n",
      "|US1WIAD0016| FRIENDSHIP 6.4 NNE|   WI|\n",
      "|US1WIAD0017|   FRIENDSHIP 6.4 N|   WI|\n",
      "|US1WIAS0003|    ASHLAND 0.5 WNW|   WI|\n",
      "|US1WIAS0004|    ASHLAND 0.5 WNW|   WI|\n",
      "|US1WIAS0006|       ODANAH 1.6 E|   WI|\n",
      "|US1WIAS0011|      ASHLAND 0.6 N|   WI|\n",
      "|US1WIAS0012|     MELLEN 2.4 NNW|   WI|\n",
      "|US1WIAS0017|    ASHLAND 7.2 SSE|   WI|\n",
      "|US1WIBF0002|   MONDOVI 12.1 SSE|   WI|\n",
      "|US1WIBF0003|    MONDOVI 0.5 NNE|   WI|\n",
      "|US1WIBN0001|  GREEN BAY 2.4 WNW|   WI|\n",
      "|US1WIBN0006|    SUAMICO 4.4 WNW|   WI|\n",
      "|US1WIBN0007|  GREEN BAY 6.3 WNW|   WI|\n",
      "|US1WIBN0008|      DE PERE 2.4 W|   WI|\n",
      "+-----------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim\n",
    "\n",
    "stations_data = spark.read.text(\"ghcnd-stations.txt\")\n",
    "\n",
    "stations_data = stations_data.select(\n",
    "    trim(stations_data.value.substr(1, 11)).alias(\"ID\"),\n",
    "    trim(stations_data.value.substr(42, 30)).alias(\"NAME\"),\n",
    "    trim(stations_data.value.substr(39, 2)).alias(\"STATE\")\n",
    ")\n",
    "\n",
    "wisconsin_stations = stations_data.filter(stations_data.STATE == \"WI\")\n",
    "wisconsin_stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6135618-5fad-4562-8533-0ee769a97153",
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = cass.prepare(\"\"\"\n",
    "INSERT INTO weather.stations(id, name)\n",
    "VALUES (?,?)\n",
    "\"\"\")\n",
    "for i in wisconsin_stations.collect():\n",
    "    cass.execute(ins, (i[\"ID\"],i[\"NAME\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c858b36-a593-403a-a7c5-e63aa43f2e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(count=1313)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"\"\"\n",
    "SELECT COUNT(*) FROM weather.stations\n",
    "\"\"\").one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e6b62a-58b5-4411-9d15-2cb190b124ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='AMBERG 1.3 SW')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "result = cass.execute(\"SELECT name FROM stations WHERE id = 'US1WIMR0003';\")\n",
    "result.one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0d542b3-f766-4c73-95d6-b396d2fecec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "result1 = cass.execute(\"SELECT token(id) FROM weather.stations WHERE id = 'USC00470273';\")\n",
    "result1.one()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c078f18-40f4-49a4-aab7-9194df36525c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-9197859302526404361',\n",
       " '-8983259405592665694',\n",
       " '-8661290616517216419',\n",
       " '-7887217505357727405',\n",
       " '-7683145164001102343',\n",
       " '-7578836006829361063',\n",
       " '-6795920394665515145',\n",
       " '-6600690554313246988',\n",
       " '-5951543747174353388',\n",
       " '-5904510863177515214',\n",
       " '-4822056253489659859',\n",
       " '-4808468962942576925',\n",
       " '-4511275517271770148',\n",
       " '-3726014353254721570',\n",
       " '-3717171852250364665',\n",
       " '-3439774900931737336',\n",
       " '-2872795204759202909',\n",
       " '-2634717242562509310',\n",
       " '-2421923496189310320',\n",
       " '-1790340595071347554',\n",
       " '-1604251563576959526',\n",
       " '-1432526974856619669',\n",
       " '-528495682351379987',\n",
       " '-361026358516586857',\n",
       " '-350072365168764315',\n",
       " '207420010359513653',\n",
       " '656825046225840158',\n",
       " '721428251171268497',\n",
       " '1286792817807115311',\n",
       " '1474496978838190952',\n",
       " '1739279655913695512',\n",
       " '2530312823487707283',\n",
       " '2550252860063770491',\n",
       " '2556951588526046307',\n",
       " '3286168552774664132',\n",
       " '3497677066566242782',\n",
       " '3632707469751625846',\n",
       " '4365541360222265790',\n",
       " '4368623162462519487',\n",
       " '5087681619080141420',\n",
       " '5447995969910121145',\n",
       " '5609061365902857762',\n",
       " '6576425608981393261',\n",
       " '6691515975590713117',\n",
       " '6706704914777184717',\n",
       " '7658880218669248616',\n",
       " '7684850367293298793',\n",
       " '8166430161495291899',\n",
       " '8166430161495291899']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "output = check_output([\"nodetool\", \"ring\"]).decode()\n",
    "output\n",
    "\n",
    "toks = []\n",
    "for line in output.split('\\n'):\n",
    "    if line.strip() and \"Datacenter\" not in line and \"Address\" not in line and \"==========\" not in line and \"Warning\" not in line and \"instead\" not in line:\n",
    "        parts = line.split()\n",
    "        toks.append(parts[-1])\n",
    "\n",
    "toks = sorted(toks, key=int)\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da790921-17e5-4be8-ab4c-afa00bb74987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7658880218669248616"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "token = result1.one()[0]\n",
    "# token = 7000000000000000000\n",
    "token = int(token)\n",
    "if token is not None:\n",
    "    if token > int(toks[-1]):\n",
    "        nextToken = toks[0]\n",
    "    nextToken = -9999999999999999999999\n",
    "    for token1 in toks:\n",
    "        if int(token) <= int(token1):\n",
    "            nextToken = int(token1)\n",
    "            break\n",
    "\n",
    "    if nextToken is None:\n",
    "        nextToken = toks[0] if toks else None\n",
    "    \n",
    "int(nextToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e90f4d1c-656e-484b-94dd-b2cc154357dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('records.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89c1c192-c705-430e-8ad6-fd18f43c6057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"records.parquet\")\n",
    "filtered_df = df.filter(col(\"element\").isin(\"TMIN\", \"TMAX\"))\n",
    "pivoted_df = filtered_df.groupBy(\"station\", \"date\").pivot(\"element\").agg({\"value\": \"first\"})\n",
    "\n",
    "processed_df = pivoted_df.withColumnRenamed(\"TMIN\", \"tmin\").withColumnRenamed(\"TMAX\", \"tmax\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90e128e7-58e1-42d9-aa26-773aa253ecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+------+\n",
      "|    station|      date|  tmax|  tmin|\n",
      "+-----------+----------+------+------+\n",
      "|USW00014898|2022-01-07| -71.0|-166.0|\n",
      "|USW00014839|2022-05-23| 150.0|  83.0|\n",
      "|USW00014839|2022-09-24| 194.0| 117.0|\n",
      "|USR0000WDDG|2022-11-30| -39.0|-106.0|\n",
      "|USR0000WDDG|2022-01-19| -56.0|-178.0|\n",
      "|USW00014839|2022-05-29| 261.0| 139.0|\n",
      "|USW00014839|2022-10-19|  83.0|  11.0|\n",
      "|USW00014837|2022-02-22| -38.0| -88.0|\n",
      "|USR0000WDDG|2022-02-02|-106.0|-150.0|\n",
      "|USW00014839|2022-09-17| 294.0| 200.0|\n",
      "|USW00014839|2022-07-08| 222.0| 189.0|\n",
      "|USW00014839|2022-04-27|  39.0|   0.0|\n",
      "|USW00014837|2022-06-24| 322.0| 200.0|\n",
      "|USW00014898|2022-01-29| -60.0|-116.0|\n",
      "|USW00014839|2022-07-15| 233.0| 156.0|\n",
      "|USR0000WDDG|2022-01-30| -33.0|-117.0|\n",
      "|USR0000WDDG|2022-02-24| -61.0|-128.0|\n",
      "|USR0000WDDG|2022-04-14|  50.0| -17.0|\n",
      "|USW00014898|2022-07-28| 256.0| 156.0|\n",
      "|USW00014837|2022-08-02| 306.0| 150.0|\n",
      "+-----------+----------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "processed_df = processed_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyyMMdd\"))\n",
    "\n",
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97c5a646-cebb-40c5-833b-46366fbe1014",
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2024-04-19T04:25:52.853759159+00:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m processed_df\u001b[38;5;241m.\u001b[39mcollect():\n\u001b[1;32m      9\u001b[0m     request \u001b[38;5;241m=\u001b[39m station_pb2\u001b[38;5;241m.\u001b[39mRecordTempsRequest(\n\u001b[1;32m     10\u001b[0m         station\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m         date\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misoformat(),\n\u001b[1;32m     12\u001b[0m         tmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmin\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     13\u001b[0m         tmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmax\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecordTemps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1161\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1148\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1155\u001b[0m     (\n\u001b[1;32m   1156\u001b[0m         state,\n\u001b[1;32m   1157\u001b[0m         call,\n\u001b[1;32m   1158\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1159\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[0;32m-> 1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1004\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2024-04-19T04:25:52.853759159+00:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "for row in processed_df.collect():\n",
    "    request = station_pb2.RecordTempsRequest(\n",
    "        station=row['station'],\n",
    "        date=row['date'].isoformat(),\n",
    "        tmin=int(row['tmin']),\n",
    "        tmax=int(row['tmax'])\n",
    "    )\n",
    "    response = stub.RecordTemps(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069a792-8834-45d2-939f-d962a113232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5\n",
    "request = station_pb2.StationMaxRequest(station=\"USW00014837\")\n",
    "response = stub.StationMax(request)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77210f9-045c-450b-a15d-f16b17805d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6\n",
    "dfSpark = spark.read.format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\") \\\n",
    "    .option(\"keyspace\", \"weather\") \\\n",
    "    .option(\"table\", \"stations\") \\\n",
    "    .load()\n",
    "\n",
    "dfSpark.createOrReplaceTempView(\"stations\")\n",
    "\n",
    "tables = spark.catalog.listTables()\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f3319-120b-4d86-a27d-0f600c3aa648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT id AS station, AVG(record.tmax - record.tmin) AS avg_diff\n",
    "FROM stations\n",
    "WHERE record.tmin is not Null\n",
    "GROUP BY id\n",
    "\"\"\"\n",
    "result_df = spark.sql(q)\n",
    "\n",
    "result_dict = {row['station']: row['avg_diff'] for row in result_df.collect()}\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca47a88-0612-4fb2-b025-a198f9297fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931b4ef-0a9f-43e7-b9fa-aa4083e9ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q9\n",
    "request = station_pb2.StationMaxRequest(station=\"USW00014837\")\n",
    "response = stub.StationMax(request)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9daaf99-e6b2-4ee6-846f-34a4c6a480d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q10\n",
    "request = station_pb2.RecordTempsRequest(\n",
    "    station=\"USW00014837\",\n",
    "    date=\"2023-06-24\",\n",
    "    tmin=-10,\n",
    "    tmax=400\n",
    ")\n",
    "response = stub.RecordTemps(request)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2500c6-c8bd-4677-be6e-cb9ac4941f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
